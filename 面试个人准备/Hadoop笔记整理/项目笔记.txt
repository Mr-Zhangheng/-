1、项目中数据来源
2、数据格式
3、数据如何采集的 
4、数据清洗做了哪儿些工作


1、pc端和app端的日志
前端js埋点收集了4个事件的数据：lantch、pv、chargerequest、event
后端javaSDK两个事件chargesuccess、chargerefund
2、数据都产生在NGINX服务器上，数据格式为以^A分隔，ip、时间戳、主机名、uri
3、flume 监控一个目录 /data/log,nginx服务器产生的日志数据按照一定大小切分后移动到/data/log
然后flume将数据拉取到HDFS上，HDFS上的目录是/logs/%m/%d,文件前缀可以通过拦截器添加的内容去动态获取
HDFS上文件的大小（一般不超过2G）
日志的回滚策略：
a、时间
b、文件大小
c、event的数量

flume的数据安全的问题：
配置事物
汇聚节点（高可用）
taildir(记录消费的数据的位置)
使用file channel

4、数据清洗将IP转换为地域、时间戳转换成毫秒，浏览器信息的解析，去除脏数据、爬虫数据，将清洗后的数据存在HDFS上
存成格式化的数据


5、数据分析：用户模块、浏览器模块、外链、订单、事件。。。。。
用户模块：时间维度和平台维度
新增用户：lantch事件的UUID的去重个数

//数据分析结果存在HDFS上，可以使用sqoop将数据导入mysql中

6、自定义输出格式化类，将mr的输出直接写入mysql
自定义输出格式化类应该怎么写？
继承outputformat
重写方法

public void checkOutputSpecs(JobContext context) throws IOException, InterruptedException {
	//检查输出路径
	//do nothing 
}

//提交给框架，原样拷贝过来就可以
public OutputCommitter getOutputCommitter(TaskAttemptContext context) throws IOException, InterruptedException {
    return new FileOutputCommitter(FileOutputFormat.getOutputPath(context), context);
}
getRecordWriter()
一个RecordWriter的子类OutputToMysqlFormat（要考虑哪儿些属性：connection、Configuration{sql语句}、将reduce输出的key转换成维度ID的接口）
子类的write方法
context.write(k,v)===>
getRecordWriter()===>
返回一个RecordWriter的子类对象(已经封装了数据库的连接，SQL语句，语句的赋值)===》
调用子类对象的write方法将数据写出



StatsUserDimension

IDimension:获取维度ID的接口
IDimensionImpl是IDimension的实现类
重写getIDimensionByObject(BaseDimension baseDimension)方法从维度表中查询维度ID（缓存）
通过维度对象从缓存中获取维度ID,
若缓存中不存在去维度表中查询，返回维度ID，将维度ID加入缓存
若维度表中查询不到，说明该维度第一次出现，将该维度插入到维度表中，返回维度ID，将维度ID加入缓存


1、数据的产生
2、数据的收集
3、数据的清洗
4、数据分析（MR,hive）
5、数据的应用






























