# 1 数据同步

## 1.0 总述

数据采集分为: 日志采集和数据库数据同步

数据同步技术: 不同系统间的数据流转

​	主数据库与备份数据库之间的数据备份

​	主系统与子系统之间的数据更新

​	同类型不同集群数据库之间的数据同步

​	不同地域,不同数据库类型之间的数据传输交换

对于大数据系统来讲, 包括:

​	数据从业务数据库同步进入数据仓库

​	数据从数据仓库同步进入数据服务或数据应用

## 1.1 数据同步基础

### 1.1.0  总述

业务类型的数据类型多样:

​	来源于关系型数据库的结构化数据

​	来源于菲关系型数据库的非结构化数据, 这类数据通常存储在数据库表中

​	来源于文件系统的结构化/非结构化数据, 如阿里云对象存储OSS,文件存储NAS,这类数据通常以文件形式进行存储

数据同步需针对不同数据类型及不同业务场景选择不同的同步方式:

​	直连同步; 数据文件同步; 数据库日志解析同步

### 1.1.1 直连同步

1. 直连同步是指通过定义好的规范接口API和基于动态链接库的方式直接连接业务库,如ODBC/JDBC等规定了统一规范的标准接口, 不同的数据库基于这套标准接口提供规范的驱动, 支持完全相同的函数调用和SQL实现
2. 配置简单, 实现容易. 较适合操作性业务数据库的数据同步, 但对原系统的性能影响较大,但执行大批量数据同步时会业务系统的性能 -- 若业务库采取主备策略,则可以从备库抽取数据, 避免对业务系统产生性能影响 ---但是当数据量较大时,采取此抽取方式性能较差,不太适合从数据系统到数据仓库系统的同步(适合到数据服务的同步)

### 1.1.2 数据文件同步

1. 通过约定好的文件编码,大小,格式等, 直接从源系统生成数据的文本文件, 有专门的文件服务器(如FTP服务器)传输到目标系统后, 加载到目标数据库中
2. 当数据源包含多个异构的数据库系统(如Mysql,DB2等)时, 用这种方式较为简单,实用; 互联网的日志类数据,通常是以文本文件形式存在的,也适合使用数据文件同步方式(例如parquet格式文件)
3. 通过文件服务器上传下载可能会造成丢包/错误, 为确保数据文件同步的完整性,通常除了上传数据文件本身外还会上传一个校验文件;该校验文件记录了数据文件的数据量以及文件大小等校验信息, 以供下游目标系统校验数据同步的准确性
4. 从源系统生成数据文件的而过程中,可以增加压缩和加密功能, 传输到目标系统后, 再对数据进行解压缩和解密--可以大大提高文件的传输效率和安全性 ;参照用户画像

### 1.1.3 数据库日志解析同步

主流数据库均已实现使用日志文件进行系统恢复--日志文件信息丰富,数据格式稳定,可以通过解析日志文件获取发生变跟的数据,从而满足增量数据的同步,实现步骤如下三步:

1. 写操作: 通过源系统的进程, 读取归档日志文件用以收集变化的日志信息, 并判断日志中的变更是否属于被收集对象,将其解析到目标数据文件中--读操作是在操作系统层面完成的, 不需要通过数据库, 因此不会给源系统带来性能影响
2. 通过网络协议(TCP/IP), 实现源系统和目标系统之间的数据文件传输. 相关进程可以确保数据文件的正确接收和网络数据包的正确顺序,并提升网络传输冗余,以确保数据文件的完整性
3. 数据文件被传输到目标系统后,可通过数据加载模块完成数据的载入, 从而实现数据从源系统到目标系统的同步

该同步方式实现了实时与准实时同步的能力,延迟在毫秒级, 对业务系统的性能影响较小 --目前广泛应用于从业务数据系统到数据仓库系统的增量数据同步应用中,参考DataX

数据库日志抽取一般是获取所有的数据记录的变更(增Insert,删Delete,改Update), 落地到目标表时需要根据逐渐去重按照日志时间倒排序获取最后状态的变化情况.--对于删除数据的变更方式,针对不同业务场景可以采用不同的落地手法.

1. 不过滤删除日志流水 -- 不管是否是删除操作,都获取同一主键最后变更的那条流水
2. 过滤最后一条删除流水 -- 如果同一主键最后变跟的那条流水是删除流水,就获取倒数第二条流水
3. 过滤删除流水和之前的流水 --如果在同一主键变更的过程中有删除操作,则根据操作时间将该删除操作对应的流水和之前的流水都过滤掉

根据前端删除无效数据的方式决定采用哪种方式处理删除数据; 前端删除数据方式一般有两种: 

1. 正常业务数据删除和手工批量删除
2. 手工批量删除针对类似的场景, 业务系统只做逻辑删除,不做物理删除, DBA定期将部分历史数据直接删除或备份到备份库
3. 一般可以采取不过滤的方式来处理, 下游通过是否删除记录的标识来判断纪录是否有效
4. 如果明确业务数据不存在业务上的删除, 但是存在批量手工删除或备份数据库删除(例如淘宝商品,会员) 则可以采用只过滤最后一条删除流水的方式, 通过状态字段来标识删除记录是否有效

该同步方式性能好, 效率高 ,对业务系统的影响较小

该方式存在的问题:

1. 数据延迟: 业务数据做批量补录可能会使数据更新量超过系统处理峰值,导致数据延时
2. 投入较大: 需要在源数据库和目标数据库之间部署一个系统实时抽取数据
3. 数据漂移和遗漏: 数据漂移一般是对增量表而言, 通常是指该表的同一个业务日期数据中包含前一天或者后一天凌晨附近的数据或者丢失当天的变更数据

## 1.2 阿里数据同步方式

### 1.2.0 总述:

1. 数据仓库的特性之一是 集成 --将不同的数据来源,不同形式的数据整合在一起; 从不同业务系统将各类数据源同步到数据仓库是一切的开始
2. 阿里数据仓库的数据同步特点:
   1. 数据来源的多样性: 来源于关系型数据库, 数据高度结构化,易于被处理; 非结构化数据--Web服务器产生的日志,各类照片,视频等,--特别是日志数据,通常直接以文本文件形式记录在文件系统中(如我们可以从用户浏览,点击页面的日志数据分析出用户的偏好和习惯,进而推荐适合的产品以提高成交率)
   2. 数据量大: 传统数据仓库系统每天同步的数据量一般在几百G甚至更少,而一些大型互联网公司的大数据系统每天同步的数据量则达到了PB乃至EB级别
3. 数据源类型多样, 需要同步的数据量大, 需要针对不同的数据源类型和数据应用的时效性要求采用不同的策略以便准确高效的完成数据同步

### 1.2.1 批量数据同步(离线)

1. 离线的数据仓库应用: 需要将不同的数据源批量同步到数据仓库; 将经过数据仓库处理的结果数据定时同步到业务系统
2. 数据仓库系统是集成各类数据源的地方,所以数据类型是统一的; 要实现各类数据库系统与数据仓库系统之间的批量双向数据同步, 需先将数据转换为中间状态, 统一数据格式 --> 可以通过将各类源数据库的数据类型统一转换为字符串类型的方式实现数据格式的统一(即dataX的value类型均为String即可) --因为此类数据都是结构化的,且均支持标准的SQL语言查询, 所以所有的数据类型都可以转换为字符串类型
3. DataX是一个满足多方向高自由度的异构数据交换服务产品,对于不同的数据源, DataX通过插件的形式提供支持, 将数据从数据源读出并转换为中间状态, 同时维护好数据的传输缓存等; 数据在DataX中以中间状态存在, 并在目标数据系统中将中间状态的数据转换为对应的数据格式后写入  ;  参考广告投放项目
4. DataX采用Framework+ Plugin的开放式框架实现.  Framework处理缓冲,流程控制,并发,上下文加载等高速数据交换的大部分技术问题,并提供简单的接口与插件接入;  插件仅需实现对数据处理系统的访问,编写方便, 开发者可以再极短时间内开发一个插件以快速支持新的数据库或文件系统; 数据传输在单进程(单机模式)/多进程(分布式模式)下完成,  --传输过程全内存操作, 不读写磁盘, 也没有进程间通信  --实现了在易购数据库或文件系统之间的高速数据交换
5. DataX架构: 
   1. Job : 数据同步作业
   2. Splitter: 作业切分模块,讲一个大任务分解为多个可以并发运行的小任务
   3. Sub-Job: 数据同步作业被切分后的小任务---Task
   4. Reader(在ThreadPool中): 数据读入模块, 负责运行切分后的小任务,将数据从源系统装载到DataX
   5. Channel: Reader和 Writer通过Channel交换数据
   6. Writer: 数据写出模块, 负责将数据从DataX导入目标数据系统

### 1.2.2 实时数据同步 ---???如何回补数据

1. 在实时中, 日志需要尽快以数据流的方式不间断的同步到数据仓库; 一些数据应用需要对业务系统产生的数据进行实时的处理, 至少实现秒级的数据刷新; 这类数据是通过解析Mysql的binlog日志(相当于Oracle的归档日志)来实时的获得增量的更新,并通过消息订阅模式来实现数据的实时同步的
2. 具体做法: 建立一个日志数据交换中心, 通过专门的模块从服务器读取日志数据; 或者解析业务数据库系统的binlog/归档日志,  最终实现将增量数据以数据流的方式不断同步到日志交换中心(TT/Kafka) ,然后通知所有订阅了这些数据的数据仓库系统来获取. --TT/Kafka具有高性能实时性顺序性高可靠性高可用性可扩展性等特点
3. TT是基于生产者,消费者和Topic消息标识的消息中间件,将消息数据持久化到HBASE的高可用分布式数据交换系统
4. 角色:
   1. 生产者: 消息数据的产生端, 向TT/Kafka发送消息数据
   2. 消费者: 消息数据的接收端, 从TT/Kafka中获取数据进行业务处理
   3. Topic: 消息类型的标识, 如淘宝acookie日志的Topic为taobao_acookie, 生产者和消费者都需知道对应的Topic名字
   4. Broker: 负责处理客户端收发消息数据的请求, 然后向HBASE取发数据
5. TT支持主动/被动等都多种数据订阅机制, 订阅端自动负载均衡, 消费者自己把握消费策略. 对于读写比例很高的Topic,实现读写分离--使消费不影响发送; 支持订阅历史数据,可随意设置订阅位置,方便用户回补数据; 针对订阅有属性过滤功能--用户只需关心自己需要的数据即可

## 1.3 数据同步遇到的问题与解决方案

### 1.3.1 分库分表的处理

1. 数据量飞速增加, 需要系统具有扩展能力和高并发大数据量的处理能力---通常使用分布式分库分表(物理表)方案来解决 ; 这种方案加大了数据同步处理的复杂性;---可以增加一个中间表(逻辑表),具备将分布在不同数据库中的不同表集成为一个表的能力,可以使下游应用像访问单库单表一样方便
2. 阿里的TDDL即是一个分布式数据库的访问引擎, 通过建立中间状态的逻辑表来整合统一分库分表的访问; 是在持久层框架之下, JDBC驱动之上的中间件, 与JDBC规范保持一致,功能:
   1. 有效解决了分库分表的规则引擎问题,实现了SQL解析,规则计算,表名替换,选择执行单元并合并结果集的功能
   2. 解决了数据库表的读写分离(中间表隔开了),高性能主备切换的问题, 实现了数据库配置信息的统一管理
3. 具体架构图见P40

### 1.3.2 高效同步和批量同步

1. 数据同步的方法通常是: 先创建目标表, 再通过同步工具(如sqoop/dataX)的填写数据库连接,表,字段等各种配置信息后测试完成数据同步 ---这也是DataX任务的配置过程
2. 其他高效和批量内容见P41 ---暂时不重要

### 1.3.3 增量和全量同步的合并

1. 数据量增大-->按周期全量同步的方式会影响处理效率 --因为涉及到了全量数据的传输;  可以选择每次只同步新变更的增量数据, 然后与上一个同步周期获得的全量数据进行合并,进而获得最新版本的全量数据
2. 合并数据一般采用merge方法(update+insert); 主流大数据平台均不支持update操作, 现通常方法为: 全外连接(full outer join)+数据全量覆盖重新加载(insert overwrite); 如日调度---将当天的增量数据和前一天的全量数据做全外连接(合并)--获取截止当天的最新全量数据, 重新加载最新的全量数据
3. 在大数据量下, 全量更新的性能比Update要高得多,  如果担心数据更新错误问题, 则可以采用分区方式, 每天保持一个最新的全量版本, 保留较短的生命周期(如3~7天)
4. 当业务系统的表有物理删除数据的操作, 而数据仓库需要保留所有历史数据时,也可以采用上述方法, 以便在数据仓库中永久保留最新的全量数据快照

### 1.3.4 同步性能的处理  ???

1. 数据同步任务: 针对不同数据库系统之间的数据同步问题而创建的一系列周期调度的任务 ; 一般首先需要设定首轮同步的线程数, 然后运行同步任务, 这种数据同步模式存在的问题:
   1. 用户不清楚如何设置首轮同步的线程数, 基本都会设置成一个固定的值---同步任务因得不到合理的CPU资源而影响同步效率
   2. 有些数据同步任务的总线程数达不到用户设置的首轮同步的线程数时, 若同步控制器将这些同步线程分发到CPU比较繁忙的机器上,则任务的平均同步速度非常慢,数据同步速度非常慢
   3. 不同的数据同步任务的重要程度是不同的, 但同步控制器平等对待接收到的同步线程---重要的同步线程因得不到CPU资源而无法同步
   4. 上述问题最终导致数据同步任务运行不稳定
2. 阿里基于负载均衡思想构建了新型数据同步方案, 核心思想是:
   1. 通过目标数据库的元数据估算同步任务的总线程数
   2. 通过系统预先定义的期望同步速度估算首轮同步的线程数
   3. 通过数据同步任务的业务优先级决定同步线程的优先级
   4. 最终实现同步任务的执行效率和稳定性
3. 具体实现步骤如下:
   1. 用户创建数据同步任务, 并提交该同步任务
   2. 根据系统提前获知和设定的数据, 估算该同步任务需要同步的数据量,平均同步时间,首轮运行期望的线程数,需要同步的总线程数
   3. 根据需要同步的总线程数将待同步的数据拆分为等量的数据块, 一个线程处理一个数据块, 并将该任务对应的所有线程提交至同步控制器
   4. 同步控制器判断需要同步的总线程数是否大于首轮运行期望的线程数
   5. 同步控制器采用多机多线程的数据同步模式, 准备该任务第一轮线程的调度, 优先发送等待时间最长, 优先级最高且同一任务的线程
   6. 同步控制器准备一定数据量(期望首轮线程数-总线程数)的虚拟线程, 采用单机多线程的数据同步模式,准备该任务实体线程和虚拟线程的调度, 优先发送等待时间最长, 优先级最高且单机CPU剩余资源可以支持首轮所有线程数且同一任务的线程, 如果没有满足条件的机器, 则选择CPU剩余资源最多的机器进行首轮发送
   7. 数据任务开始同步, 并等待完成
   8. 数据任务同步结束

### 1.3.5 数据漂移的处理

ODS: 从源系统同步进入数据仓库的第一层数据;  数据漂移是ODS数据的顽疾,通常指 ODS表的一个业务日期数据中的包含前一天或后一天凌晨附近的数据或者丢失当天的变更数据

物理落地到数据仓库的ODS表应按时间段来切分进行分区存储---因为ODS层需要承接面向历史的细节数据查询需求; 通常的做法是 按某些时间戳字段来切分 , 实际上由于时间戳字段的准确性问题会导致数据漂移

时间戳字段分类:

1. modified_time : 数据库表中用来标识数据记录更新时间的时间戳字段
2. log_time: 数据库日志中用来标识数据记录更新时间的时间戳字段
3. proc_time: 数据库表中用来记录具体业务过程发生时间的时间戳字段
4. extract_time: 标识数据记录被抽取到时间的时间戳字段

理论上,四个时间戳时间应是一致的, 但实际中会有差异,原因:

1. 数据抽取需要时间,extract_time一般时间最晚
2. 前台数据系统手工订正数据时未更新modified_time
3. 因为网络/系统压力,log_time和modified_time晚于proc_time

通常一般根据上述的其中一个字段来切分ODS表-->导致产生数据漂移; 数据漂移的场景:

1. 根据extract_time来获取数据---数据漂移问题最明显
2. 根据modified_time限制: 实际中最常见, 往往会发生不更新modified_time而导致的数据遗漏; 凌晨时间产生的数据记录漂移到后一天
3. 根据log_time限制. 由于网络或者系统压力,log_time会晚于proc_time--->凌晨时间产生的数据记录漂移到了后一天
4. 根据proc_time限制. 根据proc_time限制来获得的ODS表只是包含一个业务过程所产生的记录, 会遗漏很多的其他过程的变化记录---这违背了ODS和业务系统保持一致的设计原则

处理的方法:

1. 多获取后一天的数据:在ODS每个时间分区中向前,向后多冗余一些数据, 保障数据只会多不会少, 具体的数据切分让下游根据自身不同业务场景用不同的业务时间proc_time来限制; 缺陷是: 存在数据误差:后一天的数据可能已经更新多次, 直接获取到的那条数据已经是更新多次后的状态, 数据的准确性存在一定的问题---例如下单后隔了一天取消订单则此纪录的订单状态会被更新,下游在统计支付订单状态时会出错
2. 通过多个时间戳字段限制时间来获取相对准确的数据:
   1. 首先根据log_time分别冗余前一天最后15min的数据和后一天凌晨开始的15min的数据, 并用modified_time过滤掉非当天数据, 确保数据不会因为系统问题而遗漏
   2. 根据log_time获取后一天15min的数据,按照主键根据log_time做升序排列去重---因为我们需要获取的是最接近当天记录变化的数据---首次数据变更的记录(数据库日志将保留所有变化的数据,但落地到ODS表的是根据主键去重获得最后状态变化的数据)
   3. 将前两步结果数据做全外连接,通过限制业务时间proc_time来获取需要的数据
   4. 例如:零点支付行为包括下单,支付,成功三个业务,均有各自的时间戳字段,并不是只有支付数据发生漂移

# 2 离线数据开发

## 2.1 数据开发平台

1. 数据研发岗的工作: 了解需求--> 模型设计-->ETL开发--> 测试--> 发布上线--> 日常运维--> 任务下线
2. 提交代码前的规则校验:
   1. 代码规范类规则: 表命名规范; 生命周期设置; 表注释
   2. 代码质量类规则: 调度参数使用检查; 分母为0提醒; NUll值参与计算影响结果提醒; 插入字段顺序错误
   3. 代码性能类规则: 扫描大表提醒; 重复计算检测
3. 数据仓库的数据清洗: 采用非侵入式的清洗策略, 在数据同步过程中不进行数据清洗, 避免影响数据同步的效率, 其过程在数据进入ODS层之后执行; 若清洗掉的数据量大于预设的阈值,则阻断任务
4. 数据质量检测: 对修改前后的数据
   1. 数据对比: 
      1. 表级对比规则: 数据量和全文对比
      2. 字段级对比规则: 字段的统计值(如SUM,AVG,MAX,MIN等),枚举值,空值,去重数,长度值等
   2. 数据分布: 提取表和字段的一些特征值,并将这些特征值与预期值进行比对
      1. 表级数据特征提取主要包括数据量,主键等
      2. 字段级数据特征提取主要包括字段枚举值分布,空值分布,统计值,去重数,长度值等
   3. 数据脱敏:敏感数据模糊化

## 2.2 任务调度系统

### 2.2.0 总述

调度任务形成有向无环图

传统方法: 依靠Crontab定时任务功能进行任务调度处理; 其弊端包括:

1. 各任务之间的依赖给予执行时间实现,容易造成前面的任务未结束或失败而后面的任务已经运行
2. 任务难以并发执行, 增加了整体的处理时间
3. 无法设置任务优先级
4. 任务的管理维护很不方便, 无法进行执行效果分析

### 2.2.1 调度系统

1. 调度配置: 常见方式为对具体任务手工配置依赖的上游任务---此方法基本可以满足调度系统的正常运行; 此方法存在的问题: 
   1. 配置上较麻烦, 需要知道上游依赖表的产出任务
   2. 上游任务修改不再产出依赖表或者本身任务不在依赖某上游任务时, 对调度系统不做修改, 导致依赖配置错误
2. 定时调度: 任务的运行时间共有5种时间类型: 分钟,小时,日,周,月---具体可精确到秒; 对于周任务和月任务 , 通常选择定时调度方式
3. 周期调度: 与定时任务的区别是无需指定具体的开始运行时间 --- 离线数据处理的大多数日任务就是此类调度; 任务根据依赖关系, 按照调度数的顺序从上依次向下执行, 每个周期的具体运行时间随着每天资源和上游依赖任务运行的总体情况会有所不同
4. 补数据: 有些表需要初始化, 比如某些日增量表要补齐最近三年的历史数据,此时需要用到补数据任务;  设定需要的时间区间,并圈定需要运行的任务节点, 从而生成一个补数据的工作流, 可选择并行的运行方式以节约时间

# 3 实时技术

## 3.1 总述

1. 数据时效性分为三类:
   1. 离线--延迟粒度是天
   2. 准实时--延迟粒度是小时
   3. 实时--延迟粒度是秒
   4. 离线和实时都可以在批处理系统中实现,只是调度周期不一样而已; 实时数据在流式处理系统中完成---业务系统每产生一条数据就会立刻被采集并实时发送到流式任务中进行处理, 不需要定时调度任务来处理数据
2. 流式处理的特点: 常驻进程任务,数据源是无界的

## 3.2 流式技术架构

### 3.2.1 数据采集

1. 从采集的数据种类来看,可以划分为: 
   1. 数据库变更日志: 比如MySQl的binlog日志,HBASE的hlog日志,Oracle的变更日志
   2. 引擎访问日志: 如用户访问网站产生的Apache引擎日志;搜索引擎的接口查询日志
2. 两类日志都会在服务器上落地成文件---只要监控文件的内容发生变化, 采集工具就可以把最新的数据采集下来
3. 出于吞吐量和系统压力的考虑, 并不是每新增一条记录就采集一次, 而是按批次对数据进行采集,采集原则:
   1. 数据大小原则: 当达到限制条件时, 把目前采集到的新数据作为一批(如512K写一批)
   2. 时间阈值限制: 但时间达到一定条件时, 也会把目前采集到的新数据作为一批---避免在数据量少的情况下一直不采集(如30s写一批)
   3. 只要满足以上的其中一个条件,就会被作为一批新数据采集到数据中间件(如kafka中),当批次采集频繁时, 可以降低延时, 但必然会导致吞吐量下降
4. 消息系统一般会用作业务数据库的消息中转(如下单,支付等) ,对于其他较大的业务数据(几十T/天),一般会通过数据中间件来中转,其延时在秒级,但支持的吞吐量大.
5. 从消息系统中获取的数据不是最全的(如有些订单数据是通过同步方式导入Mysql中的),而通过数据库变更日志拿到的业务数据肯定是全的;  为了和离线数据源保持一致, 一般通过数据中间件来采集数据库变更数据来获取实时数据的(这需要在数据处理层对业务主键进行merge处理, 进行merge拿到多次变更之后的最新数据)

### 3.2.2 数据处理  ???

为提高性能,实时计算通常是多线程的, 一般会根据业务主键进行分桶处理, 大部分计算过程需要的数据都会存放在内存中---大大提高了应用的吞吐量;  为了避免内存溢出,内存中过期的数据需要定期清理,可按照LRU(最近最少使用)算法或业务时间集合归类清理

实时任务遇到的问题:

1. 去重指标: 在实时任务中, 对资源的消耗很高;  实时为了追求处理性能,计算逻辑(spark)是在内存中完成的, 中间结果数据也会缓存在内存中--->内存消耗过多;  在计算去重时, 会把去重的明细数据保存下来,当去重的明细数据上亿时内存放不下时,  分两种情况:

   1. 精确去重: 明细数据必须保存下来, 遇到内存问题时, 通过数据倾斜来进行处理--将一个节点的内存压力分到多个节点上
   2. 模糊去重: 去重的数据量大而业务精度要求不高时, 可使用相关的去重算法,将内存的使用量降低,以提高内存的利用率

2. 去重算法:

   1. 布隆过滤器: 位数组算法的应用,不保存真实的明细数据,只保存明细数据对应哈希值的标记位---会出现哈希值碰撞的情况,但误差率可控,计算出来的去重值比真实值小---一亿--->100多M

      适用场景: 统计精度要求不高,统计维度较多时;  如全网各个商家的UV数据 --在各个维度之间, 布隆过滤器可共用

   2. 基数估计: 利用Hash的原理, 按数据的分散程度来估算现有数集的边界-->得出大概的去重值总和; 结果可能大/小; 一亿--->几K内存

      适用场景: 统计精度要求不高, 统计维度非常粗时;  如整个大盘的UV数据-->每天只有一条数据; 若统计全天小时的UV数据,则需要24个基数估计对象;  ---基数估计在各个维度之间不可共享

3. 数据倾斜: ETL中常见,数据量较大时,单个节点的处理能力是有限的-->性能瓶颈;  需要对数据进行分桶处理---分桶处理和离线处理思路一致

   1. 去重指标分桶: 通过对去重值进行分桶Hash,相同的值一定会放在一个桶中去重,再将每个桶中的值进行加和就可得到总值---利用了每个桶的CPU和内存资源
   2. 非去重指标分桶: 数据随机发到每个桶中, 再将每个桶的值汇总--主要利用了每个桶的CPU能力

4. 事务处理: 实时计算均为分布式处理---系统不稳定-->数据发送失败;数据丢失; 如何做到数据的精确处理: 数据自动ACK,失败重发,事务信息等机制:

   1. 超时时间: 数据处理是按照批次进行的, 一批数据处理时间超时-->数据重发; 批次处理的数据量不宜过大--应当加一个限流的功能(限定一批数据的记录数或容量等)--避免数据处理超时
   2. 事务信息: 每批数据都会附带一个事务ID的信息, 在重发的情况下, 开发者自己根据事务信息去判断数据第一次到达和重发时不同的处理逻辑
   3. 备份机制: 要保证内存数据可以通过外部存储恢复--在计算中用到的中间结果数据需要备份到外部存储
   4. 以上机制保证了数据的幂等性

### 3.2.3 数据存储 ???

1. 中间计算的很多维度和指标,需要保存在一个存储系统中作为恢复或者关联使用; 涉及到三种类型的数据:
   1. 中间计算结果: 会有一些状态的保存(如 去充指标的明细数据)--用于发生故障时,舒勇数据库中的数据恢复内存现场
   2. 最终结果数据: 通过ETL处理后的实时结果数据--这些数据是实时更新的, 写的频率非常高,可以被下游直接使用
   3. 维表数据: 在离线中通过同步工具导入到在线存储系统中, 供实时任务来关联实时流数据
2. 多线程处理-->数据存储系统必须支持多并发读写,延时需要在毫秒级才能满足实时任务; 一般使用HBASE,MongoDB等列式存储系统---在写数据时是先写内存再写磁盘,因此写延时在毫秒级; 读请求有缓存机制,多并发读时也可以达到毫秒级延时
3. 上述数据库缺点: 以HBASE为例: 
   1. 一张表必须要有行键,而rowKey是根据ASCLL码排序的, 就想MYsql的索引一样,rowkey的规则限制了读取数据的方式;  如果业务方需要使用另一种读取数据的方式,就必须重新输出rowkey
   2. HBASE的一张表能够存储NT的数据,而关系型数据库必须数据存储的此量级才能实现分库分表
   3. 对于海量数据的实时计算,一般采用非关系型数据库--应对大量的多并发读写
4. 表名设计: 
   1. 规则: 层级标识+数据域(业务主题)+主维度(主体)+时间维度; 如 dws_trd_slr(卖家)_dtr(截止当日)
   2. 如此设计的好处是: 所有主维度相同的数据都放在一张物理表中, 可以避免表数据过多,难以维护; 表名形象,见名知意
5. rowkey设计: 
   1. MD5+主维度+维度标识+子维度1+时间维度+子维度2; 如 卖家ID的MD5前四位 + 卖家ID(主维度) + APP(维度标识) + 一级类目ID + ddd + 二级类目ID 
   2. 优点: 以MD5的前四位作为rowkey的第一部分---可以将数据散列, 让服务器整体负载均衡, 避免热点问题;  解释: 卖家ID属于主维度,在查询数据时是必传的;  每个统计维度都会生成一个维度标识---以便在rowkey上做区分

## 3.3 流式数据模型

### 3.3.0 总述

1. 对数据流建模分层: ODS,DWD,DWS,ADS,DIM ;  实时计算性能要求过高--每一层的表没有离线时那么宽,且基本不涉及回溯状态的指标

### 3.3.1 数据分层

1. ODS: 最原始数据,包含所有业务的变更过程,数据粒度是最细的; 实时与离线 数据源头一致--->同一份数据,指标口径统一, 方便实时与离线间的数据对比
2. DWD: 根据业务过程建模出来的实时数据明细表, 如: 订单的支付明细表,退款明细表;用户的访问日志明细表
3. DWS: 计算各个维度的汇总指标; 若维度是通用的则放在通用汇总层,汇总指标各个业务线是公用的;  电商的极大维度的汇总表---卖家/买家/商品
4. ADS: 个性化维度汇总层,  计算只有自身业务才会关注的维度和指标,与其他业务线无交集
5. DIM: 
6. 举例说明: 
   1. ODS: 订单粒度的变更记录 ---一条订单有多条记录
   2. DWD: 订单粒度的支付记录---一笔订单只有一条记录
   3. DWS: 卖家的实时成交金额---一个卖家只有一条记录,且指标在实时刷新
   4. ADS: 外卖地区的实时成交金额---只有外卖业务使用
   5. DIM: 订单商品类目和行业的对应关系维表
7. ods离线加工生成dim; ods和dim汇合生成dwd; ods和dwd层在数据中间件中, 由下游订阅使用;  dws和ads会落地到在线存储系统,下游通过接口调用使用

### 3.3.2 多流关联

1. 将两个实时流进行主键关联-->得到对应的实时明细表;  因为数据的到达是增量的过程且数据到达的时间是不确定的和无序的---> 中间状态的保存和恢复机制等
2. 过程: 订单表和支付表-->根据ID实时关联-->查找(订单表某数据到达,到支付表截止当前的全量数据中查找)--若找到,则关联成功,拼接成一条数据进入下游;若未找到,关联失败,则需要放在内存中自己表数据的集合中或外部存储中等待 --->关键点是数据的相互等待;  无论是否关联成功,内存中的数据都需要备份到外部存储系统中--->任务重启时,可从外部存储系统中恢复内存数据,保证数据不丢失--->因为在重启时,任务是续跑的,不会重新跑之前的数据
3. 订单记录的变更可能发生多次(如 订单的多字段更新),---需要根据订单ID去重,避免A表和B表多次关联成功,防止数据重复
4. 实时关联时一般会将数据按照关联主键进行分桶处理, 在故障恢复是也根据分桶进行--降低查找数据量和提高吞吐量

### 3.3.3 维表使用

1. 根据业务分区来关联事实表和维表, 在关联前维表数据已就绪;在实时中,关联维表一般会使用当前的实时数据(T) 去关联T-2的维表数据,相当于在T的数据到达前需要把维表数据准备好--一般是一份静态的数据

2. 数据无法及时准备好: 当零点到达时,实时数据必须去关联维表(不能等待--保证实时性) ,但是T-1的维表数据一般无法在零点马上准备就绪(因为T-1的数据需要在T这一天加工完成),因此去关联T-2维表---在T-1的一天时间里加工好T-2的维表数据

3. 无法获取全量的最新数据: 维表一般是全量的,若需要实时获取到当天的最新维表数据,需要T-1的数据+当天变更 -->完整的维表数据;  此时维表也作为一个实时流输入--使用多流实时关联来实现---因为实时数据是无序的且到达时间是不确定的--->在维表关联上有歧义

4. 数据的无序性: 若维表作为实时流输入,获取维表数据将存在困难. 实时应用永远也不知道什么时候才是最新状态--不知道维表后面是否会发生变更;

   在实时计算中维表关联一般采用T-2数据--对于业务来说,起码关联到的维表数据是确定的(虽然维表数据有一定的延时,但一般的业务维表在两天之间变化是很少的)

   在有些场景下, 可以关联T-1表, 但是T-1的数据是不全的, 如在22点开始对维表数据加工处理,但最终会缺失2个小时的维表变更过程

5. 实时任务是常驻内存进程的---维表的使用分为两种:

   1. 全量加载: 维表数据较少时, 可一次性加载到内存中, 在内存中直接和实时流数据进行关联, 效率非常高, 

      内存一直占用, 并且需要实时更新

      如: 类目信息, 每天几万条记录 ,在每天零点时全量加载到内存中

   2. 增量加载: 使用增量查找和LRU过期的形式, 让最热门的数据留在内存中

      优点: 可控制内存使用量, 

      缺点: 需要查找外部存储系统, 运行效率会降低

      如: 会员维度,实时数据到达,查询外部存储数据,将查询结果放入内存,定期清理数据,避免内存溢出

## 3.4 大促挑战和保障

### 3.4.1 大促特征

1. 毫秒级延时(高吞吐,低延时)
2. 洪峰明显 --- 全链路压测
3. 高保障性 --多链路冗余(从采集--处理--数据服务  整个链路做物理隔离)

### 3.4.2 大促保障

1. 实时任务优化: 
   1. 合理选择缓存机制,尽量降低读写库次数(内存读写性能最好,可将最热和最可能的数据留在内存中,降低读写库的次数)
   2. 计算单元合并,降低拓扑层级(数据在每个节点的传输一般都需序列化和反序列化--该过程非常消耗CPU和时间)
   3. 在高吞吐和低延时建取舍(将多个读写库操作或ACK操作合并成一个--降低网络请求的消耗,延时变大)
   4. 独占资源和共享资源的策略
2. 数据链路保障: 实时数据处理链路: 数据同步-->数据计算-->数据存储-->数据服务; 分布式计算单节点故障时常态-->多链路搭建,多机房(机器)容灾,异地容灾
3. 压测: 模拟峰值,验证系统是否可正常运行,都是在线上环境中进行--分为数据压测和产品压测
   1. 数据压测: 蓄洪压测--将几天的积累的数据在某时刻全部放开,模拟峰值
   2. 产品压测

# 4 数据服务

## 4.0 总述

1. 收敛接口: 将数据按照其统计粒度进行聚合, 同样维度的数据,形成一张逻辑表,采用同样的接口. 如: 会员维度:将所有以会员为中心的数据做成一张逻辑表,只要是查询会员粒度的数据,仅需要调用会员接口即可

2. 成交回头率: 统计时间周期内,有两笔及以上成交父订单的买家数除以有成交父订单的买家数
3. 缓存优化: 结果缓存
   1. 某些查询较复杂, 可以将结果进行缓存, 下次执行相同的查询时, 可直接从缓存中获取结果
   2. 获取某个卖家所属类目的统计指标, 一个类目下有上万个卖家, 每个卖家请求的结果肯定是完全一致的, 可将结果放在缓存中, 大部分请求可直接在缓存中得到结果--缓存命中率超级高 ---而且类目记录数不多,不会增加过多的额外开销
4. 稳定性: 服务启动时会将元数据全量加载至本地缓存;  变更安全保障:
   1. 元数据隔离: 日常环境: 线下开发  预发环境:正式发布前校验即将上线的代码   线上环境   分别对应:日常元数据,预发元数据,线上元数据
5. 限流: 当某个数据源查询激增 超过预设的QPS阈值,则后续的请求里脊失败返回,不在继续执行 保障系统的可用性
6. 数据源出现故障--降级处理-->限流,将QPS设置为0,防止故障扩散

# 5 数据挖掘 

## 5.0 用户画像  P123

# 6 建模综述

## 6.1 数据建模原因

从业务,数据存取和使用角度合理组织和存储数据

好处: 

1. 性能: 减少数据的I/O吞吐
2. 成本: 减少不必要的数据冗余,实现计算结果复用---降低存储和计算成本
3. 效率: 提高数据使用效率
4. 质量: 改善数据统计口径的不一致性,减少数据计算错误的可能性

## 6.2 OLTP和OLAP的区别

1. OLTP(联机事务处理): 主要数据操作是 随机读写 ,主要采用满足3NF的试题关系模型存储数据, 从而在事务处理中解决数据的冗余和一致性问题 --主要针对数据库的日常事务处理;实时性高,数据量不大,高并发
2. OLAP(联机分析处理): 主要数据操作是 批量读写,事务处理的一致性不是OLAP所关注的---主要关注数据的整合和在一次性的复杂大数据查询和处理的性能 --主要针对数据仓库应用,实时性不高,数据量大--支持动态查询,维度概念很重要

## 6.3 维度模型

1. 维度建模: 从分析决策的需求出发构建模型,为分析需求服务, 典型代表: 星型模型, 部分特殊场景可考虑雪花模型
2. 步骤: 
   1. 选择需要进行分许决策的业务过程; 单个业务事件(交易的支付,退款),某事件的状态(当前账号的余额),一系列相关业务事件组成的业务流程
   2. 选择粒度: 需预判所有分析需要细分的程度,从而决定选择的粒度---粒度是维度的一个组合
   3. 识别维度: 选好粒度后, 需要基于此粒度设计维表--包括维表属性,用于分析时进行分组和筛选
   4. 选择事实,确定分析需要衡量的指标

# 7 数据整合及管理体系

## 7.1 概述

1. 避免数据的冗余和重复建设以及数据的不一致性

### 7.1.1 体系架构

1. 规范定义层: 
   1. 数据域--->[修饰类型(修饰词) + 业务过程(原子指标+度量) + 维度(维度属性)]
   2. 派生指标: 原子指标+时间周期+修饰词
2. 模型设计层: 
   1. 维表: 把逻辑维度物理化的宽表 --- 维度
   2. 明细事实表: 最原始粒度的明细数据 --- 度量+维度
   3. 汇总事实表: 将明细事实聚合的事实表 ---派生指标+维度

## 7.2 规范定义

### 7.2.1 名词术语

1. 0-0: 电商业务

2. 1-0: 数据域(交易域): 面向业务分析,将业务过程或维度进行抽象的集合---需长期维护和更新,但不轻易变动

3. 2-0 业务过程(支付): 一个不可拆分的行为**事件** ,如 下单,支付,退款等

   2-1 维度(订单): 反映业务的一类属性,其集合构成一个维度(实体对象),属于一个数据域, 如地理维度,时间维度

4. 3-0 修饰类型(支付方式):  从属于某个数据域,如日志域的访问终端类型,

5. 4-0 时间周期(最近一天): 明确数据统计的时间范围或时间点

   4-1 修饰词(花呗):除了统计维度以外指标的业务场景限定抽象,如PC端,无线端

   4-2 原子指标/度量(支付金额): 不可再拆分的指标,具有明确业务含义的名词

6. 派生指标: 对原子指标业务统计范围的圈定

### 7.2.2 指标体系

1. 命名规范见广告投放业务解释文档

2. 派生指标分类: 

   1. 事务型指标: 对业务活动进行衡量

      如: **新发商品数,重发商品数,新增注册会员数,订单支付金额**

   2. 存量型指标: 对实体对象(如**商品,会员**)某些状态的统计

      如: **商品总数,注册会员总数**,对应的时间周期一般为**历史截至当前某个时间**

   3. 复合型指标: 上述两者的复合

      如: **浏览UV-下单买家数转化率**

3. 复合型指标的规则: 

   1. 比率型: 如**CTR,浏览UV-下单买家数转化率,满意率; ---最近一天店铺首页CTR**
   2. 比例型: 如**百分比,占比; ---最近一天无线端支付金额占比**
   3. 变化量型: 如**最近一天订单支付金额上一天变化量**
   4. 变化率型: 如**最近7天海外买家支付金额上7天变化率**
   5. 统计型: 如**均值,分位数 ---人均,日均,行业平均,商品平均,90分位数,70分位数**
   6. 排名型: 如**升/降序,名次,范围(行业/省份/来源),依据(PV,搜索次数...)**

4. 其他规则:

   1. 上下层级派生指标同时存在: 建议使用上层指标;下层指标作为维度属性存放在物理表中

## 7.3 模型设计

### 7.3.1 模型层次 ???

1. 模型架构: 图见P150

   1. 应用-->ODS(离线数据/准实时数据)--> DW(公共维度层)
   2. DW--DWD: 明细宽表层-->面向业务过程建模(事务型事实宽表; 周期性快照事实宽表; 累计快照事实宽表)
   3. DWD-->DWS:公共汇总宽表层-->面向分析主题建模(买家/卖家/买卖家; 商品/全站/行业; 会员/地区...)

2. 操作数据层ODS: 

   1. 同步: 结构化数据增量/全量同步到Hive
   2. 结构化: 非结构化(日志)结构化处理并存储到Hive
   3. 数据清洗/保存历史数据

3. 公共维度模型层(DW): 

   1. 存放明细事实表,维表数据,公共指标汇总数据;

      前两者由ODS层数据处理获得

      后者由前两者处理获得

   2. 一般采用维度退化手法,将维度退化至事实表中, 减少事实表和维表的关联,提高明细数据表的易用性;  在汇总数据层,加强指标的维度退化,采取更多的宽表化手段构建公共指标数据层--->提升公共指标的复用性,减少重复加工; 主要功能是: 

      1. 组合相关和相似数据---采用明细宽表,复用关联计算,减少数据扫描
      2. 公共指标统一加工: 建立逻辑汇总宽表
      3. 建立一致性的分析维度

4. 数据应用层ADS: 

   1. 个性化指标: 非公用性,复杂性(指数型; 比值型; 排名型)
   2. 基于应用的数据组装: 大宽表集市
   3. 构建全域的公共层数据---控制数据规模的增长趋势

5. 优先调用公共维度模型(DW),当公共层没有数据时,需要评估是否需要创建公共层数据;  当不需要建设公用的公共层时,才可以直接使用ODS层数据

### 7.3.2 基本原则

1. 高内聚低耦合: 将业务相近或者相关, 粒度相同的数据设计为一个逻辑/物理模型; 将高概率同时访问的数据放在一起,将低概率同时访问的数据分开存储
2. 核心模型与扩展模型(一般为个性化/少量应用的)分离,减少干扰,提高可维护性
3. 成本与性能唯一:适当的数据冗余
4. 数据可回滚: 在不同时间多次运行数据结果确定不变
5. 一致性: 相同含义的字段在不同表中的命名必须相同,规范命名--见名知意

### 7.3.3 模型实施（数仓建设流程）

1. 架构设计: 

   1. 数据域划分：数据域指面向业务分析， 将业务过程或者维度进行抽象的集合

      如根据业务线划分为：

      商品管理-->商品上下架,商品名称/类目修改

      会员管理-->新增会员,会员登录,会员信息修改

      评价管理--> 好中差评;评分

      物流库存管理-->入出库,发货,签收

      客户反馈-->投诉,举报

      用户行为追踪-->商品/店铺浏览,网页区块点击

      交易流程管理-->下单,订单支付,确认收货,退货,退款

      抽象出的对应的数据域:

      会员和店铺域--> 注册,登录,装修,开店,关店

      商品域--> 发布,上下架,重发,SKU库存

      日志域--> 曝光,浏览,点击

      交易域--> 加购,下单,支付,退款,确认收货

      服务域--> 商品收藏,优惠券领用,

      互动域--> 发帖,回帖,评论,评价

   2. 构建矩阵:

      明确每个数据域下有哪些业务过程,业务过程与哪些维度相关;  同时定义每个数据域下的业务过程和维度

   3. 规范定义:

      主要定义指标体系--包括原子指标,时间周期,派生指标等

   4. 模型设计:

      主要包括:维度及属性的规范定义, 维表,明细事实表和汇总事实表的模型设计

# 8 维度设计

## 8.1 维度设计基础

### 8.1.1 维度基本概念

1. 将度量称为事实,将环境称为维度---维度即为分析事实所需要的多样环境; 

   维度所包含的表示维度的列,称为维度属性;

   维度一般是查询约束,分类汇总(分组),排序以及报表标签

2. 维度使用主键标识其唯一性---主键是确保与之相连的事实表之间存在引用完整性的基础

   主键分为代理键和自然键 ---用于标识某维度的具体值

   代理键: 不具有业务含义的键,一般用于处理缓慢变化维

   自然键: 具有业务含义的键

   如 对于商品维表的每一行,可以生成一个唯一的代理键与之对应,商品本身的自然键可能是商品ID等

### 8.1.2 维度的基本设计方法

维度的设计过程就是确认维度属性的过程--- 如何生成维度属性,生成的维度属性的优劣

以商品维度为例: 

1. 选择维度或新建维度: 必须保证维度的唯一性, 以商品维度为例,有且只有一个维度定义
2. 确定主维表: 一般为ODS表---直接与系统同步
3. 确定相关维表: 数仓--数据整合和表之间的关联性--->确定哪些表和主维表存在关联关系,选其中某些表生成维度属性,  如商品与类目,SPU,卖家,店铺等维度存在关联关系
4. 确定维度属性: 从主维表选择维度属性或生成新的维度属性;从相关维表选择维度属性或生成行的维度属性

确定维度属性的注意点:

1. 尽可能生成丰富的维度属性 ---方便下游的数据统计分析查找

2. 尽可能多的给出包括一些有意义的文字性描述 ; 如商品维度的商品ID和商品名称---ID一般用于表之间的关联,名称一般用于报表标签

3. 区分数值型字段的属性: 作定语--事实;作修饰语--维度

   用于查询条件/分组统计--维度,一般为离散值

   参与度量的计算--事实,一般为连续值

4. 尽量沉淀出通用的维度属性:

   对于比较复杂的逻辑计算;通过多表关联获得;通过单表的不同字段混合处理获得;通过单表的某个字段解析获得 --->封装为单独的维度属性--->较少复杂性;避免下游解析因各自的逻辑不同而口径不一致

### 8.1.3 维度的层次结构

1. 维度中的属性一般以层次方式或一对多的方式相互关联---可应用于数据钻取

   以商品为例: 类目,行业,品牌等属性层次既可以实例化为多个维度,也可以作为维度属性存在于商品维度中

### 8.1.4 规范化和反规范化

1. 当属性层次被实例化为一系列维度,而不是单一维度时---雪花模型-->大部分OLTP的底层数据结构采用的这种规范化技术-->可将重复属性转移至自身所属的表中,删除冗余字段--->在OLTP中可有效避免数据冗余导致的不一致性
2. 将维度的属性层次合并到单个维表中的操作--反规范化;  分析系统主要是数据分析和统计---雪花模型会使用大量的关联操作,复杂度高,查询性能差
3. 对于OLAP来说,数据是稳定的,雪花模型仅仅可以节约一部分的存储---现阶段存储成本非常低,所以一般维表都是不规范化的---易用性和性能

### 8.1.5 一致性维度和交叉探查

1. 数仓总线架构基础之一: 一致性维度--->针对不同数据域进行迭代/并行构建时,现将不同数据域(日志域和交易域)的商品的事实合并在一起进行数据探查,如 计算转化率---交叉探查
2. 交叉探查错误一般因为: 维度格式和内容不一致两种类型
3. 维度一致性表现形式:
   1. 共享维度:  商品,卖家,买家,类目等维度有且只有一个---属于公共维度
   2. 一致性上卷: 商品维度包含类目维度
   3. 交叉属性: 两个维度具有部分相同的维度属性,基于该维度属性进行交叉探查

## 8.2 维度设计高级主题

### 8.2.1 维度整合

1. 数据仓库的定义: 是一个面向主题的, 集成的, 非易失的且随时间变化的数据集合
2. 数仓的集成性体现在:从大量的分散的数据源中
   1. 统一的命名规范(表名,字段名等)
   2. 字段类型的统一; 相同和相似字段的字段类型统一
   3. 公共代码和代码值的统一
   4. 业务含义相同的表的统一:依据高内聚低耦合---将业务关系大,源系统影响差异小的表进行整合, 反之进行分而置之
3. 集成的方式:
   1. 主从表的设计方式: 将共有字段放在主表中(主要基本信息),从属信息放在各自的从表中,主表中的主键--采用复合主键---源主键和系统/表区别标识
   2. 直接合并: 共有信息和个性信息均放在一个表中,若字段重合度较低时--出现大量空值
   3. 不合并: 若差异过大无法合并时--用多个表分别存放即可
4. 表级别的整合有两种表现形式:
   1. 垂直整合: 不同的来源表包含相同的数据集,只是存在的信息不同; 如会员有基础信息表,扩展信息表,等级信息表等,应尽量整合至会员维度中
   2. 水平整合: 不同的数据来源包含不同的数据集,不同子集无交集; 水平整合时,若存在交叉,则需考虑不同子集的自然数=键是否存在冲突---若不冲突,可将各子集的自然键作为整合后的自然键/设置超自然键---可以考虑将来源字段作为分区字段

### 8.2.2 水平拆分

1. 维度通常按照类别/类型 进行细分
2. **设计维度**的两种方案: 
   1. 将维度的不同分类实力华为不同的维度,同时在主维度中保存公共属性
   2. 维护单一维度没包含所有可能的属性
3. 在数据模型设计过程中需考虑三个原则: 
   1. 扩展性: 当源系统,业务逻辑发生变化时, 通过少量的成本快速扩展模型,保持核心模型的相对稳定---高内聚低耦合
   2. 效能: 牺牲一定的存储成本,获取性能和逻辑的优化**
   3. 易用性: 可理解性高,访问复杂度低
4. 根据数据模型设计思想, 对维度进行水平拆分时的两个依据: 
   1. 维度的不同分类的属性差异情况:  当维度属性随类型变化较大时, 将所有可能的属性建立在一个表中是不合适的, 此时建议用方案一; 如建立一个主维度用来存放公共属性(一般较稳定),多个子维度用来存放各自的特殊属性---通过扩展子维度的方式,保证魔性的稳定性
   2. 业务的关联程度:  相关性较低的业务,耦合弊大于利, 对模型的稳定性和易用性影响较大

### 8.2.3 垂直拆分

1. 维度属性的丰富程度直接决定数仓的能力
2. 依据维度设计的原则,尽可能丰富维度属性, 同时进行反规范化处理---进行**维度设计**;  其中遇到的问题: 
   1. 维度分类的不同而存在特殊的维度属性---水平拆分解决
   2. 扩展性,产出时间,易用性等的不同---设计主从维度,垂直拆分解决
      1. 主维表存放稳定,产出时间早,热度高,使用频繁的属性
      2. 从维表存放变化较快,产出时间晚,热度低的属性
      3. 从维表中有冗余的变化较快的数据属性,对于主维表进行缓慢变化的处理很重要---通过存储的冗余和计算成本的增加--实现商品主模式的稳定和产出时间的提前

### 8.2.4 历史归档--历史维表

1. 归档策略有三种: 
   1. 同前台归档策略--在数据仓库中实现前台归档算法,定期对历史数据进行归档---前台归档策略复杂,实现成本高; 前台归档策略可能经常变化--导致数据仓库归档算法也随之改变,维护成本高  ---> 适用于逻辑较为简单,且变更不频繁的情况
   2. 同前台归档策略--但采用数据库变更日志的方式;  日志抽取策略为通过数据库binlog日志解析获取每日增量,通过增量merge全量的方式获取最新全量数据---不需关注前台归档策略,简单易行---> 要求前台数据库的删除只有在归档时才执行,应用中的删除只是逻辑删除
   3. 自定义归档策略--原则是尽量比前台应用晚归档,少归档---避免出现数仓中已经归档的数据再次进行更新的情况
   4. 建议使用策略2 

## 8.3 纬度变化

### 8.3.1 缓慢变化维

1. 数仓的特点之一就是反应历史变化;维度的变化相对缓慢; 事实表的数据增长较快
2. **处理维度变化**的三种方式: 
   1. 重写维度值 ---不保留历史数据,始终取得最新数据
   2. 插入新的维度行--- 保留历史数据,纬度值变化前的事实和过去的纬度值关联,纬度值变化后的事实和当前的维度值关联
   3. 添加维度列: 方法二不能解决将变化前后记录的事实归一为变化前的维度或归一为变化后的维度; 法三则可以保留历史数据,可以使用任何一个属性列
   4. 最终选择哪种方案, 根据业务需求来进行选择, 如 是否关系历史数据

### 8.3.2 快照维表

**如何处理缓慢变化维**: 

1. 使用代理键作为每个维表的主键:  ---弊端:
   1. 对于分布式计算系统,不存在事务的概念; 对于每个表的记录生成稳定(某条记录每次生成的代理键都相同)的全局唯一的代理键难度很大
   2. 使用代理键会大大增加ETL的复杂性,对ETL任务的开发和维护成本很高
2. 使用快照方式:  数仓的计算周期一般每天一次,基于此周期,处理维度变化的方式就是每天保留一份全量快照数据--通过限定日期,采用自然键进行关联即可
3. 优点: 
   1. 简单有效,开发维护成本低
   2. 使用方便,理解性好---数据使用只需限定日期,即可获得当天的快照数据, 任意一天的事实快照和维度快照通过维度的自然键进行关联即可
4. 弊端:
   1. 主要体现在极大地浪费上: 若某维度,每天的变化量站总体数据量的比例很低,极端情况每天无变化时,是的存储浪费严重---实现了牺牲存储获取ETL效率的优化和逻辑上的简化
   2. 必须要杜绝过度使用此方法,必须要有对应的数据生命周期制度,清除无用的历史数据
5. 在当前存储成本远低于CPU,内存等的成本下,此方法利大于弊
6. 极限存储可降低存储,用保留快照维表的优点

### 8.2.3 极限存储

1. 历史拉链存储: 利用维度模型中缓慢变化维的第二种方式---新增两个时间戳字段(start_dt,end_dt) -- 将以天为粒度的变更数据都记录下来---通常分区字段也是时间戳字段
2. 采用历史拉链表时,对于不变的数据,不会再重复存储;  可以通过限制时间戳字段来获取历史数据;  但是此存储方式对下游使用存在一定的理解障碍,同时采用start_dt和end_dt做分区,随着时间的推移---分区数量会极度膨胀---一般的数据库系统都有分区数量限制
3. 改进:  采用分月做历史拉链表: 一年历史拉链表最多会产生的分区数: 365*364/2=66430个--->改用每月初重新开始历史拉链表,则一年最多产生分区数12*(1+(30+29)/2)=5232
4. 极限存储:压缩了全量存储的成本,但对于变化频率高的数据并不能达到节约成本的效果---变化频繁的字段需要过滤,否则极限存储就相当于每个分区存储一份全量数据,不会节约存储  ----可采用垂直拆分/微型维度的方式解决

### 8.3.4 微型维度 : 略

1. 递归层次: 均衡层次结构(有固定数量的级别); 非均衡层次结构;  

   在递归层次中进行上钻和下钻很常见

2. 层次结构扁平化(仅针对固定数量的级别): 降低递归层次使用复杂度 ---可在一定程度上解决上钻和下钻的问题

   如 对于类目维度: 每个类目保存一条记录,并将其所属的各类目层级属性化,没有的用空值填充

3. 层次桥接表---略

### 8.4.1 特殊维度

1. 行为维度: 略

2. 多值维度: 

   事实表的一条记录在某维表中有多条记录与之对应,如一个买家一次购买了多种商品---订单表中的一条记录在商品表中会有多条记录--->针对多值维度,常见的处理方式: 

   1. 降低事实表的粒度: 将订单设计为子订单粒度--对于每个子订单粒度,只有一种商品与之对应,其中的事实则分摊到子订单中  ---  但是很多时候事实表的粒度不可降低
   2. 采用多字段: 如夫妻合买的合同签订事实表,每条记录存在多个买受方,而 合同 已经是此事时中的最细粒度  ---  由于合同签订的买受方一般不会太多,所以一般采用多字段方式 ---若考虑到扩展性,则可通过预留字段的方式 过多时填写为其他受买方即可
   3. 采用通用的桥接表: 方式更灵活,扩展性更好,但逻辑复杂,开发和维护成本较高 --- 可能会有双重计算的风险   --- 一般叫做分组表,若事实表一条记录对应两个买受方,则桥接表建立两条记录,分组Key相同  --->分组计算不一定错误  若两个买受方籍贯不同则可能会分别统计--双重计算

3. 多值属性: 维表的某属性字段同时有多个值; 对于多值属性,处理方式有三种:

   1. 保持维度主键不变,将多值属性放在维度的一个属性中 --如 可通过K-V对的形式放在某字段中 --- 扩展性好,但是数据使用较麻烦
   2. 保持维度主键不变, 将多值属性放在维度的多个属性字段中; 如卖家主营类目可能有多个,但业务只是取得TOP3, 此时维度的多值属性字段具体值的数量固定-->可采用多个属性字段进行存储; 若数量不固定, 则可采用预留字段的方式---扩展性较差
   3. 维度主键发生变化,一个纬度值存放多条记录; 如商品SKU维表,对于每个商品,有多少SKU,就有多少记录---主键是商品的ID和SKU的ID  -->此种方法扩展性好,使用方便, 但是需要考虑数据的急剧膨胀情况

# 9 事实表设计

## 9.1 事实表基础

### 9.1.1 事实表特性

1. 事实表围绕业务过程来设计, 通过获取描述业务过程的度量来表达业务过程, 包含了引用的维度和与业务过程有关的度量
2. 事实表中一条记录所表达的业务细节程度被称为粒度,  粒度的两种表述方式:
   1. 维度属性组合所表示的细节程度
   2. 所表示 的具体业务含义
3. 事实一般为整型或浮点型的十进制数值, 有三种类型:
   1. 可加性事实: 可按照与事实表关联的任意维度进行汇总
   2. 半可加性事实: 只能按照特定维度汇总,不能对所有维度汇总  -- 比如库存一般不会按照时间维度将一年内每个月的库存累加
   3. 完全不具备可加性:  比如比率型事实  --- 对于不可加性事实可分解为可加的组件来实现聚集
4. 事实表一般更细长,行的增加速度比维表快很多;  维度属性可以存储到事实表中, 存储到事实表中的维度列被称为 退化维度 ,与维度表中的维度一样,退化维度也可用来进行事实表的过滤查询,实现聚合操作
5. 事实表有三种类型: 
   1. 事务事实表: 用来描述业务过程, 跟踪空间或时间上某点的度量事件,保存的是最原子的数据 --- 原子事务表
   2. 周期快照事实表: 以具有规律性的,可预见的时间间隔记录事实,时间间隔如 每天,每月,每年等
   3. 累积快照事实表: 用来表述过程开始和结束之间的关键步骤事件,覆盖过程 的整个生命周期, 通常具有多个日期字段用来记录关键时间点---当过程随着生命周期不断变化时,记录也会随着过程的变化而被修改

### 9.1.2 事实表设计原则

1. **尽可能包含所有与业务过程相关的事实**:  事实表设计的目的是度量业务过程 --- 分析哪些事实表和业务过程有关; 该原则---即使存在冗余,但因为事实通常为数字型,带来的存储开销也不会很大
2. **只选择与业务过程相关的事实**: 如在订单的下单这个业务过程的事实表设计中,不应存在支付金额这一表示支付业务过程的事实
3. **分解不可加性事实为可加的组件**: 如优惠率---应分解为订单原价金额和订单优惠金额两个事实存储在事实表中
4. **在选择维度和事实之前必须先声明粒度**: 粒度用于确定事实表中一行所表示业务的细节层次,决定了维度模型的扩展性,每个维度和事实必须和所定义的粒度保持一致; --- 粒度一般定义的越细越好, 建议从最低级别的原子粒度开始---> 原子粒度提供了最大限度的灵活性,可支持无法预期的各种细节层次的用户需求;  在事实表中,通常使用业务来描述粒度,但对于聚集性事实表的粒度描述,可采用维度或维度组合的方式
5. **在同一个事实表中不能有多种不同粒度的事实**: 如 机票支付成功事务事实表,粒度为票一级,一个订单可能有多张票,所以支付金额与该票级事实表的粒度不一致,且不能进行汇总---若该两个维度在该表进行汇总计算总订单金额和总票数则会造成重复计算
6. **事实的单位要保持一致**: 计量单位要统一
7. **对事实的null值要处理**: 在数据库中null值对常用数字型字段的SQL过滤条件都不生效---(>,<,<=,>=,<>), 建议用零值填充
8. **使用退化维度提高事实表的易用性**:维度的获取采用的是通过事实表的外键关联专门的维表的方式;  大量的采用退化维度,在事实表中存储各种类型的常用维度信息---为了减少下游用户使用时关联多个表的操作,直接通过退化维度实现对事实表的过滤查询,控制聚合层次,排序数据以及定义主从关系等--->通过增加冗余存储的方式减少计算开销,提高使用效率

### 9.1.3 事实表设计方法

1. **选择业务过程和确定事实表类型**: 明确业务需求---> 进行详细的需求分析,对业务的整个生命周期进行分析,明确关键的业务步骤--->选择与需求有关的业务过程
   1. 业务过程通常使用 行为动词 表示业务执行的活动,  在明确了订单流程所包含的业务过程后,需要根据具体的业务需求来选择与维度建模有关的业务过程
   2. 选择了业务过程之后,相应的事实表类型也随之确定了, 如选择买家付款这个业务, 则事实表为只包含买家付款这一业务过程的单事务事实表; 若选择的是四个业务过程,且需分析各个业务过程之间的时间间隔,则事实表应为包含四个业务过程的累积快照事实表
2. **声明粒度**: 粒度的声明意味着精确定义事实表的每一行所表示的业务含义,粒度传递的是与事实表有关的细节层次 --- 明确的力度可以确保对事实表中行的意思的理解不会差生混淆,保证所有的事实都按照同样的细节层次记录--->应选最细级别的原子粒度
3. **确定维度**: 粒度声明完成-->确定了主键--->对应的维度组合和相关的维度字段也可以确定了 --- 应该选择能够描述清楚业务过程所处环境的维度信息, 如 订单付款事实表中,力度为子订单,相关的维度有买家,卖家,商品,收货人信息,业务类型,订单时间等维度
4. **确认事实**: 事实可以通过回答**过程的度量是什么**来确定, 应选择与业务过程有关的所有事实,且事实的粒度要和所声明的事实表的粒度一致,注意不可加性事实的分解
5. **冗余维度**: 对维度的处理一般是需要单独存放在专门的维表中的,通过事实表的外键获取维度---这样做是为了减少事实表的冗余维度,从而减少存储消耗;---在大数据的事实表模型设计中,考虑更多的是提高下游用户的使用效率,降低数据的获取复杂度,减少关联的表数量--->所以通常事实表中会冗余方便下游用户使用的常用维度, 如订单付款事实表中,通常会冗余大量的常用维度字段,以及商品类目,卖家店铺等维度信息

## 9.2 事实事务表

订单是交易行为的核心载体,下单,支付,成功完结三个业务过程是整个订单的关键节点,获取此三个业务过程的笔数,金额,转化率是重点,采用了事务事实表

### 9.2.1 设计过程

0. 任何类型的事件都可以被理解为一种事务,如创建订单,买家付款,申请退款都可以理解为一种事务;  事务事实表---针对这些过程构建的一类事实表,用以跟踪定义业务过程的个体行为,作为数仓原子的明细数据
1. **选择业务过程** :包括 下单,支付,发货,成功完结 四个业务过程 --- 不仅是交易过程中的重要时间节点,而且是下游统计分析的重点,  便于进行独立的分析研究 --- 是否将不同业务过程放到同一个事实表中
2. **确认粒度**: 业务过程选定之后,针对每一个业务过程确认一个粒度---确认事实事务表每一行所表达的细节层次 ---> 首先需要详细分析其订单交易的产生过程--卖家类型下单方式父子订单 --- 若同一个店铺只购买了一种商品则父子订单将会合并,只保留一条订单记录  ---> 下单,支付,成功完结选择交易子订单粒度;卖家发货更多的是物流单粒度而非子订单粒度---同一个子订单可拆分为多个物流单进行发货,所以确定为物流单粒度
3. **确定维度**: 按照经常用于统计分析的场景, 确认为杜包括:卖家,买家,商品,商品类目,发货地区,收获地区,父订单维度,杂项维度;  订单的属性较多--订单的业务类型,是否无线交易,是否秒杀等---使用较多但有无法归属到上述买卖家或商品维度中的属性--->新建一个杂项维度进行存放
4. **确定事实**: 事实表应包括所有事实,如交易事务事实表--三个业务过程(下单,支付,成功完结),不同的业务过程有不同的事实;  由于粒度是子订单,对于父订单上的金额需分摊到子订单上
5. **冗余维度**: 处于效率和资源的考虑,将常用维度全部退化到事实表中,使下游分析使用模型更方便; 传统星型建模理论建议在事实表中只保存这些维表的外键

### 9.2.2 单事务事实表

1. 每个业务过程设计一个事实表 --- 可以方便的对每个业务过程进行独立的分析研究

### 9.2.3 多事务事实表

1. 将不同的事实放到统一个事实表 --- 同一个事实表中包含不同的业务过程 ;  多事实事实表在设计时进行事实的处理有两种方式: 
   1. 不同业务过程的事实使用不同的事实字段进行存放
   2. 使用同一个事实字段进行存放,但增加一个业务过程标签
2. 示例一 : 交易事务事实表(多事务)
   1. 采用将不同业务过程的事实使用不同事实字段进行存放的设计模式 ---下单/支付/成功完结 三个业务过程拥有相同的粒度,都是子订单粒度---比较适合放在同一个事实表中; 发货的粒度比子订单更细,属于不同粒度的业务过程---为放在一个事实表中
   2. 对于不同的业务过程和粒度,维度也不完全相同,但在此处,常用维度比较一致,如卖家,买家,商品,类目,店铺,收发货地区---故在维度层面可保证三个业务过程放到同一个事务事实表中
   3. 将多个业务过程放在同一个事实表中,将面对的是如何处理多个事实  --- 针对每个度量都使用一个字段进行保存, 不同的事实使用不同的字段进行存放, 若不是当前业务过程的度量---采用零值处理方式,全部置为0即可
   4. 在一个事实表中如何标注多个业务过程: 针对每个业务过程打一个标签, 标记当天是否是这个业务过程,标签之间互不相关
3. 示例二: 收藏商品事务事实表
   1. 两个业务过程(动作): 收藏商品/删除商品
   2. 确定粒度  ---> 用户的粒度+商品的粒度
   3. 粒度决定了-->维度 主要也是用户维度和商品维度; 为了使事实表信息更丰富---> 冗余了商品类目维度和商品所属卖家维度  ---两个业务过程的维度是一致的
   4. 相同的粒度和维度--->多事务事实表; 只是在不同业务过程的事实上进行区别, 此处采用使用同一个字段存放不同业务过程的事实---使用标签字段区分不同的业务过程,如用 收藏事件类型 字段区分  ---更多的是无事实的事实表,通常用于统计收藏/删除的次数
4. 多事务事实表的选择:
   1. 当不同业务过程的度量比较相似,差异不大时使用第二种方案---在同一周期内会存在多条记录
   2. 当不同业务过程的度量差异较大时 --- 采用第一种方案---度量值零值较多

### 9.2.4 两种事务事实表的对比

0. 单  多
1. 业务过程: 一个  多个
2. 粒度: 相互间不相关  相同粒度
3. 维度: 相互间不相关  一致
4. 事实: 只取当前业务过程的事实  保留多个业务过程的事实,非当前业务过程中的事实需要置零处理
5. 冗余维度: 多个业务过程,则需要冗余多次  不同的业务过程只需要冗余一次
6. 理解程度: 易于理解,不易混淆  难以理解,需要通过标签来限定
7. 计算存储成本: 较多,每个业务过程都需要计算存储一次  较少,不同业务过程融合在一起,降低了存储计算量,但是非当前业务过程的度量存在大量零值

### 9.2.5 事实的设计原则

1. 事务完整性; 所有事务
2. 事务一致性: 度量单位一致
3. 事务可加性: 不加价性事务分拆

## 9.3 周期快照事实表

1. 当需要一些状态度量时, 如余额,星级,库存等则需要聚集与之相关的事务才能进行识别计算,此时事务事实表是无效率的
2. 快照事实表在确定的间隔内对实体的度量进行抽样---可以很容易的研究实体的度量值,不需要聚集长期的事务历史

### 9.3.1 特性























































































聚合和开窗的区别

幂等性

1.2.2 数据回补

1.3.1 读写分离





​	